{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":22794,"status":"ok","timestamp":1644491356501,"user":{"displayName":"thomas milton","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06460723225067031802"},"user_tz":0},"id":"0BLeaq6-ldG-","outputId":"fcae0a08-e5ca-4760-a51b-9219d46326f5"},"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[33m\r0% [Working]\u001b[0m\r            \rHit:1 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran40/ InRelease\n","\u001b[33m\r0% [Connecting to archive.ubuntu.com (91.189.88.152)] [Connecting to security.u\u001b[0m\r                                                                               \rIgn:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  InRelease\n","\u001b[33m\r0% [Connecting to archive.ubuntu.com (91.189.88.152)] [Connecting to security.u\u001b[0m\u001b[33m\r0% [1 InRelease gpgv 3,626 B] [Connecting to archive.ubuntu.com (91.189.88.152)\u001b[0m\r                                                                               \rIgn:3 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  InRelease\n","\u001b[33m\r0% [1 InRelease gpgv 3,626 B] [Connecting to archive.ubuntu.com (91.189.88.152)\u001b[0m\r                                                                               \rHit:4 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Release\n","\u001b[33m\r0% [1 InRelease gpgv 3,626 B] [Connecting to archive.ubuntu.com (91.189.88.152)\u001b[0m\r                                                                               \rHit:5 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Release\n","Hit:6 http://security.ubuntu.com/ubuntu bionic-security InRelease\n","Hit:7 http://archive.ubuntu.com/ubuntu bionic InRelease\n","Hit:8 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic InRelease\n","Hit:9 http://archive.ubuntu.com/ubuntu bionic-updates InRelease\n","Hit:10 http://archive.ubuntu.com/ubuntu bionic-backports InRelease\n","Hit:11 http://ppa.launchpad.net/cran/libgit2/ubuntu bionic InRelease\n","Hit:14 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu bionic InRelease\n","Hit:15 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic InRelease\n","Reading package lists... Done\n","Building dependency tree       \n","Reading state information... Done\n","62 packages can be upgraded. Run 'apt list --upgradable' to see them.\n","Requirement already satisfied: Box2D in /usr/local/lib/python3.7/dist-packages (2.3.10)\n","Reading package lists... Done\n","Building dependency tree       \n","Reading state information... Done\n","xvfb is already the newest version (2:1.19.6-1ubuntu4.10).\n","The following packages were automatically installed and are no longer required:\n","  cuda-command-line-tools-10-0 cuda-command-line-tools-10-1\n","  cuda-command-line-tools-11-0 cuda-compiler-10-0 cuda-compiler-10-1\n","  cuda-compiler-11-0 cuda-cuobjdump-10-0 cuda-cuobjdump-10-1\n","  cuda-cuobjdump-11-0 cuda-cupti-10-0 cuda-cupti-10-1 cuda-cupti-11-0\n","  cuda-cupti-dev-11-0 cuda-documentation-10-0 cuda-documentation-10-1\n","  cuda-documentation-11-0 cuda-documentation-11-1 cuda-gdb-10-0 cuda-gdb-10-1\n","  cuda-gdb-11-0 cuda-gpu-library-advisor-10-0 cuda-gpu-library-advisor-10-1\n","  cuda-libraries-10-0 cuda-libraries-10-1 cuda-libraries-11-0\n","  cuda-memcheck-10-0 cuda-memcheck-10-1 cuda-memcheck-11-0 cuda-nsight-10-0\n","  cuda-nsight-10-1 cuda-nsight-11-0 cuda-nsight-11-1 cuda-nsight-compute-10-0\n","  cuda-nsight-compute-10-1 cuda-nsight-compute-11-0 cuda-nsight-compute-11-1\n","  cuda-nsight-systems-10-1 cuda-nsight-systems-11-0 cuda-nsight-systems-11-1\n","  cuda-nvcc-10-0 cuda-nvcc-10-1 cuda-nvcc-11-0 cuda-nvdisasm-10-0\n","  cuda-nvdisasm-10-1 cuda-nvdisasm-11-0 cuda-nvml-dev-10-0 cuda-nvml-dev-10-1\n","  cuda-nvml-dev-11-0 cuda-nvprof-10-0 cuda-nvprof-10-1 cuda-nvprof-11-0\n","  cuda-nvprune-10-0 cuda-nvprune-10-1 cuda-nvprune-11-0 cuda-nvtx-10-0\n","  cuda-nvtx-10-1 cuda-nvtx-11-0 cuda-nvvp-10-0 cuda-nvvp-10-1 cuda-nvvp-11-0\n","  cuda-nvvp-11-1 cuda-samples-10-0 cuda-samples-10-1 cuda-samples-11-0\n","  cuda-samples-11-1 cuda-sanitizer-11-0 cuda-sanitizer-api-10-1\n","  cuda-toolkit-10-0 cuda-toolkit-10-1 cuda-toolkit-11-0 cuda-toolkit-11-1\n","  cuda-tools-10-0 cuda-tools-10-1 cuda-tools-11-0 cuda-tools-11-1\n","  cuda-visual-tools-10-0 cuda-visual-tools-10-1 cuda-visual-tools-11-0\n","  cuda-visual-tools-11-1 default-jre dkms freeglut3 freeglut3-dev\n","  keyboard-configuration libargon2-0 libcap2 libcryptsetup12\n","  libdevmapper1.02.1 libidn11 libip4tc0 libjansson4 libnvidia-cfg1-510\n","  libnvidia-common-460 libnvidia-common-510 libnvidia-extra-510\n","  libnvidia-fbc1-510 libnvidia-gl-510 libpam-systemd libpolkit-agent-1-0\n","  libpolkit-backend-1-0 libpolkit-gobject-1-0 libxi-dev libxmu-dev\n","  libxmu-headers libxnvctrl0 libxtst6 nsight-compute-2020.2.1\n","  nsight-compute-2022.1.0 nsight-systems-2020.3.2 nsight-systems-2020.3.4\n","  nsight-systems-2021.5.2 nvidia-dkms-510 nvidia-kernel-common-510\n","  nvidia-kernel-source-510 nvidia-modprobe nvidia-settings openjdk-11-jre\n","  policykit-1 policykit-1-gnome python3-xkit screen-resolution-extra systemd\n","  systemd-sysv udev xserver-xorg-core-hwe-18.04 xserver-xorg-video-nvidia-510\n","Use 'apt autoremove' to remove them.\n","0 upgraded, 0 newly installed, 0 to remove and 62 not upgraded.\n","Requirement already satisfied: pyvirtualdisplay in /usr/local/lib/python3.7/dist-packages (2.2)\n","Requirement already satisfied: EasyProcess in /usr/local/lib/python3.7/dist-packages (from pyvirtualdisplay) (1.1)\n"]}],"source":["# TD3: Using https://github.com/sfujim/TD3\n","# TD3_FORK: Using https://github.com/honghaow/FORK\n","\n","!apt update\n","!pip install Box2D\n","!apt install xvfb -y\n","!pip install pyvirtualdisplay\n","\n","import io\n","import gym\n","import time\n","import json\n","import copy\n","import glob\n","import base64\n","import collections\n","import numpy as np\n","import pandas as pd\n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","from tqdm.notebook import tqdm\n","\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torch.nn.functional as F\n","\n","from IPython.display import HTML\n","from IPython import display as disp\n","from pyvirtualdisplay import Display"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1915,"status":"ok","timestamp":1644491358408,"user":{"displayName":"thomas milton","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06460723225067031802"},"user_tz":0},"id":"hy21AhSw9Mob","outputId":"4c363c89-5862-4f41-82b7-e6fd383b8002"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"markdown","metadata":{"id":"01IaEImbyrrR"},"source":["### Helper Functions"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Wk6GhdOFyrJw"},"outputs":[],"source":["def show_video(episode):\n","    \"\"\"From https://colab.research.google.com/github/jeffheaton/t81_558_deep_learning/blob/master/t81_558_class_12_01_ai_gym.ipynb\"\"\"\n","    mp4list = glob.glob('video/*.mp4')\n","    if len(mp4list) > 0:\n","      mp4 = [i for i in mp4list if i.endswith(str(episode).zfill(6)+'.mp4')][0]\n","      video = io.open(mp4, 'r+b').read()\n","      encoded = base64.b64encode(video)\n","      disp.display(HTML(data='''<video alt=\"test\" autoplay \n","                  loop controls style=\"height: 400px;\">\n","                  <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" />\n","              </video>'''.format(encoded.decode('ascii'))))\n","\n","\n","def log_results(directory, ep_reward_list, model):\n","    with open(directory + 'logs.json', 'r+') as f:\n","      data = json.load(f)\n","\n","      try: \n","          lists = data[model]\n","          lists.append(ep_reward_list)\n","      except:\n","          data[model] = [ep_reward_list]\n","\n","      f.seek(0)\n","      json.dump(data, f)\n","\n","\n","def plot_results(ep_reward_list, label='', smooth=50, show_std=True):\n","    sns.set_theme()\n","    plt.ylabel('Episode Reward')\n","    plt.xlabel('Episode Number')\n","\n","    avg_reward_list = pd.Series(ep_reward_list).rolling(window=smooth).mean()\n","    avg_std_list = pd.Series(ep_reward_list).rolling(window=smooth).std()\n","\n","    plt.plot(range(len(avg_reward_list)), avg_reward_list, label=label)\n","    if show_std: plt.fill_between(range(len(avg_reward_list)), \n","                        [x-y for x,y in zip(avg_reward_list, avg_std_list)], \n","                        [x+y for x,y in zip(avg_reward_list, avg_std_list)], \n","                        alpha=0.2)"]},{"cell_type":"markdown","metadata":{"id":"OSjPMwUcECXr"},"source":["### Replay Buffer"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MpN0jlYBEB70"},"outputs":[],"source":["class ReplayBuffer(object):\n","    def __init__(self, device, state_dim, action_dim, max_size):\n","        self.device = device\n","        self.max_size = max_size\n","        self.ptr = 0\n","        self.size = 0\n","\n","        self.state = np.zeros((max_size, state_dim))\n","        self.action = np.zeros((max_size, action_dim))\n","        self.next_state = np.zeros((max_size, state_dim))\n","        self.reward = np.zeros((max_size, 1))\n","        self.not_done = np.zeros((max_size, 1))\n","\n","    def add(self, state, action, next_state, reward, done):\n","        self.state[self.ptr] = state\n","        self.action[self.ptr] = action\n","        self.next_state[self.ptr] = next_state\n","        self.reward[self.ptr] = reward\n","        self.not_done[self.ptr] = 1. - done\n","\n","        self.ptr = (self.ptr + 1) % self.max_size\n","        self.size = min(self.size + 1, self.max_size)\n","\n","    def sample(self, batch_size, CER=False):\n","        ind = np.random.randint(0, self.size, size=batch_size)\n","        \n","        # Replace last transition with current transition\n","        if CER: ind[-1] = ((self.ptr -1) % self.max_size)\n","\n","        return (\n","          torch.FloatTensor(self.state[ind]).to(self.device),\n","          torch.FloatTensor(self.action[ind]).to(self.device),\n","          torch.FloatTensor(self.next_state[ind]).to(self.device),\n","          torch.FloatTensor(self.reward[ind]).to(self.device),\n","          torch.FloatTensor(self.not_done[ind]).to(self.device)\n","        )"]},{"cell_type":"markdown","metadata":{"id":"Yvd1N4qUR2Oh"},"source":["### Models"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"M7WjIh5eR5MX"},"outputs":[],"source":["class Actor(nn.Module):\n","    def __init__(self, state_dim, action_dim, max_action, fc1=256, fc2=256):\n","        super(Actor, self).__init__()\n","\n","        self.l1 = nn.Linear(state_dim, fc1)\n","        self.l2 = nn.Linear(fc1, fc2)\n","        self.l3 = nn.Linear(fc2, action_dim)\n","        \n","        self.max_action = max_action\n","\t\t\n","    def forward(self, state):\n","        a = F.relu(self.l1(state))\n","        a = F.relu(self.l2(a))\n","        return self.max_action * torch.tanh(self.l3(a))\n","\n","\n","class Critic(nn.Module):\n","    def __init__(self, state_dim, action_dim, fc1=256, fc2=256):\n","        super(Critic, self).__init__()\n","\n","        self.l1 = nn.Linear(state_dim + action_dim, fc1)\n","        self.l2 = nn.Linear(fc1, fc2)\n","        self.l3 = nn.Linear(fc2, 1)\n","\n","        self.l4 = nn.Linear(state_dim + action_dim, fc1)\n","        self.l5 = nn.Linear(fc1, fc2)\n","        self.l6 = nn.Linear(fc2, 1)\n","\n","\n","    def forward(self, state, action):\n","        sa = torch.cat([state, action], 1)\n","\n","        q1 = F.relu(self.l1(sa))\n","        q1 = F.relu(self.l2(q1))\n","        q1 = self.l3(q1)\n","\n","        q2 = F.relu(self.l4(sa))\n","        q2 = F.relu(self.l5(q2))\n","        q2 = self.l6(q2)\n","        return q1, q2\n","\n","\n","class SysReward(nn.Module):\n","    def __init__(self, state_dim, action_dim, fc1=256, fc2=256):\n","        super(SysReward, self).__init__()\n","\n","        self.l1 = nn.Linear(2 * state_dim + action_dim, fc1)\n","        self.l2 = nn.Linear(fc1, fc2)\n","        self.l3 = nn.Linear(fc2, 1)\n","\n","    def forward(self, state, next_state, action):\n","        sa = torch.cat([state,next_state, action], 1)\n","\n","        q1 = F.relu(self.l1(sa))\n","        q1 = F.relu(self.l2(q1))\n","        q1 = self.l3(q1)\n","        return q1\n","\n","\n","class SysModel(nn.Module):\n","    def __init__(self, state_dim, action_dim, fc1=400, fc2=300):\n","        super(SysModel, self).__init__()\n","\n","        self.l1 = nn.Linear(state_dim + action_dim, fc1)\n","        self.l2 = nn.Linear(fc1, fc2)\n","        self.l3 = nn.Linear(fc2, state_dim)\n","\n","    def forward(self, state, action):\n","        xa = torch.cat([state, action], 1)\n","\n","        x1 = F.relu(self.l1(xa))\n","        x1 = F.relu(self.l2(x1))\n","        x1 = self.l3(x1)\n","        return x1"]},{"cell_type":"markdown","metadata":{"id":"uhBzZCcQl-uo"},"source":["### TD3"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7W2zlcYVluTz"},"outputs":[],"source":["class TD3(object):\n","    def __init__(self, device, state_dim, action_dim, max_action, lr):\n","\n","        self.actor = Actor(state_dim, action_dim, max_action).to(device)\n","        # self.actor.apply(self.init_weights)\n","        self.actor_target = copy.deepcopy(self.actor)\n","        self.actor_optimizer = torch.optim.Adam(self.actor.parameters(), lr=lr)\n","\n","        self.critic = Critic(state_dim, action_dim).to(device)\n","        # self.critic.apply(self.init_weights)\n","        self.critic_target = copy.deepcopy(self.critic)\n","        self.critic_optimizer = torch.optim.Adam(self.critic.parameters(), lr=lr)\n","\n","        self.total_it = 0\n","\n","    def init_weights(self, layer):\n","        \"\"\"Xaviar Initialization of weights\"\"\"\n","        if(type(layer) == nn.Linear):\n","          nn.init.xavier_normal_(layer.weight)\n","          layer.bias.data.fill_(0.01)\n","\n","    def select_action(self, device, state):\n","        state = torch.FloatTensor(state.reshape(1, -1)).to(device)\n","        return self.actor(state).cpu().data.numpy().flatten()\n","\n","    def train(self, device, replay_buffer, max_action, batch_size, discount, tau, policy_noise, noise_clip, policy_freq, CER=False):\n","        self.total_it += 1\n","\n","        # Sample replay buffer \n","        state, action, next_state, reward, not_done = replay_buffer.sample(batch_size, CER)\n","\n","        with torch.no_grad():\n","            # Select action according to policy and add clipped noise\n","            noise = (torch.randn_like(action) * policy_noise).clamp(-noise_clip, noise_clip)\n","            next_action = (self.actor_target(next_state) + noise).clamp(-max_action, max_action)\n","\n","            # Compute the target Q value\n","            target_Q1, target_Q2 = self.critic_target(next_state, next_action)\n","            target_Q = torch.min(target_Q1, target_Q2)\n","            target_Q = reward + (not_done * discount * target_Q)\n","\n","        # Get current Q estimates\n","        current_Q1, current_Q2 = self.critic(state, action)\n","\n","        # Compute critic loss\n","        critic_loss = F.mse_loss(current_Q1, target_Q) + F.mse_loss(current_Q2, target_Q)\n","\n","        # Optimize the critic\n","        self.critic_optimizer.zero_grad()\n","        critic_loss.backward()\n","        self.critic_optimizer.step()\n","\n","        # Delayed policy updates\n","        if self.total_it % policy_freq == 0:\n","\n","            # Compute actor loss\n","            actor_loss = -self.critic.forward(state, self.actor(state))[0].mean()\n","            \n","            # Optimize the actor \n","            self.actor_optimizer.zero_grad()\n","            actor_loss.backward()\n","            self.actor_optimizer.step()\n","\n","            # Update the frozen target models\n","            for param, target_param in zip(self.critic.parameters(), self.critic_target.parameters()):\n","              target_param.data.copy_(tau * param.data + (1 - tau) * target_param.data)\n","\n","            for param, target_param in zip(self.actor.parameters(), self.actor_target.parameters()):\n","              target_param.data.copy_(tau * param.data + (1 - tau) * target_param.data)\n","\n","    def save(self, filename):\n","        torch.save(self.critic.state_dict(), filename + \"_critic.pth\")\n","        torch.save(self.critic_optimizer.state_dict(), filename + \"_critic_optimizer.pth\")\n","        \n","        torch.save(self.actor.state_dict(), filename + \"_actor.pth\")\n","        torch.save(self.actor_optimizer.state_dict(), filename + \"_actor_optimizer.pth\")\n","\n","    def load(self, filename):\n","        self.critic.load_state_dict(torch.load(filename + \"_critic.pth\"))\n","        self.critic_optimizer.load_state_dict(torch.load(filename + \"_critic_optimizer.pth\"))\n","        self.critic_target = copy.deepcopy(self.critic)\n","\n","        self.actor.load_state_dict(torch.load(filename + \"_actor.pth\"))\n","        self.actor_optimizer.load_state_dict(torch.load(filename + \"_actor_optimizer.pth\"))\n","        self.actor_target = copy.deepcopy(self.actor)"]},{"cell_type":"markdown","metadata":{"id":"hXq7DLEH8Ab1"},"source":["### TD3-FORK"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"USS2a4bs7__-"},"outputs":[],"source":["class TD3_FORK:\n","    def __init__(self, device, state_dim, action_dim, max_action, lr):\n","        super().__init__()\n","        \n","        self.actor = Actor(state_dim, action_dim, max_action).to(device)\n","        self.actor_target = copy.deepcopy(self.actor)\n","        self.actor_optimizer = torch.optim.Adam(self.actor.parameters(), lr=lr)\n","\n","        self.critic = Critic(state_dim, action_dim).to(device)\n","        self.critic_target = copy.deepcopy(self.critic)\n","        self.critic_optimizer = torch.optim.Adam(self.critic.parameters(), lr=lr)\n","\n","        self.sys_model = SysModel(state_dim, action_dim).to(device)\n","        self.sys_model_optimizer = optim.Adam(self.sys_model.parameters(), lr=lr)\n","\n","        self.sys_reward = SysReward(state_dim, action_dim).to(device)\n","        self.sys_reward_optimizer = optim.Adam(self.sys_reward.parameters(), lr=lr)\n","        \n","        self.obs_upper_bound, self.obs_lower_bound = 0, 0\n","        self.total_it = 0\n","\n","    def select_action(self, device, state):\n","        state = torch.FloatTensor(state.reshape(1, -1)).to(device)\n","        return self.actor(state).cpu().data.numpy().flatten()\n","\n","    def train(self, device, replay_buffer, max_action, batch_size, discount, tau, policy_noise, noise_clip, policy_freq, CER=False):\n","        self.total_it += 1\n","\n","        # Sample replay buffer \n","        state, action, next_state, reward, not_done = replay_buffer.sample(batch_size, CER)\n","\n","        with torch.no_grad():\n","            # Select action according to policy and add clipped noise\n","            noise = (torch.randn_like(action) * policy_noise).clamp(-noise_clip, noise_clip)\n","            next_action = (self.actor_target(next_state) + noise).clamp(-max_action, max_action)\n","\n","            # Compute the target Q value\n","            target_Q1, target_Q2 = self.critic_target(next_state, next_action)\n","            target_Q = torch.min(target_Q1, target_Q2)\n","            target_Q = reward + not_done * discount * target_Q\n","\n","        # Get current Q estimates\n","        current_Q1, current_Q2 = self.critic(state, action)\n","\n","        # Compute critic loss\n","        critic_loss = F.mse_loss(current_Q1, target_Q) + F.mse_loss(current_Q2, target_Q)\n","\n","        # Optimize the critic\n","        self.critic_optimizer.zero_grad()\n","        critic_loss.backward()\n","        self.critic_optimizer.step()\n","\n","        ############################ New Stuff ############################\n","\n","        # Compute sys model loss\n","        predict_next_state = self.sys_model(state, action)\n","        predict_next_state = predict_next_state.clamp(self.obs_lower_bound, self.obs_upper_bound)\n","        sys_model_loss = F.smooth_l1_loss(predict_next_state, next_state.detach())\n","\n","        # Optimise sys model\n","        self.sys_model_optimizer.zero_grad()\n","        sys_model_loss.backward()\n","        self.sys_model_optimizer.step()\n","        sys_model_loss = sys_model_loss.item()\n","\n","        # Compute sys reward loss\n","        predict_reward = self.sys_reward(state,next_state,action)\n","        sys_reward_loss = F.mse_loss(predict_reward, reward.detach())\n","\n","        # Optimise sys model\n","        self.sys_reward_optimizer.zero_grad()\n","        sys_reward_loss.backward()\n","        self.sys_reward_optimizer.step()\n","        sys_reward_loss = sys_reward_loss.item()\n","    \n","        s_flag = 1 if sys_model_loss < 0.020  else 0\n","\n","        # Delayed policy updates\n","        if self.total_it % policy_freq == 0:\n","\n","            # Compute actor loss\n","            actor_loss = -self.critic.forward(state, self.actor(state))[0].mean()\n","\n","            if s_flag == 1:\n","                p_next_state = self.sys_model(state, self.actor(state))\n","                p_next_state = p_next_state.clamp(self.obs_lower_bound, self.obs_upper_bound)\n","                actions2 = self.actor(p_next_state.detach())\n","                p_next_r = self.sys_reward(state,p_next_state.detach(), self.actor(state))\n","\n","                p_next_state2 = self.sys_model(p_next_state, self.actor(p_next_state.detach()))\n","                p_next_state2 = p_next_state2.clamp(self.obs_lower_bound, self.obs_upper_bound)\n","                actions3 = self.actor(p_next_state2.detach())\n","                p_next_r2 = self.sys_reward(p_next_state.detach(), p_next_state2.detach(), self.actor(p_next_state.detach()))\n","\n","                actor_loss2 = -self.critic(p_next_state2.detach(), actions3)[0]\n","                actor_loss3 =  -(p_next_r + discount * p_next_r2 + discount ** 2 * actor_loss2).mean()\n","\n","                actor_loss = (actor_loss + 0.5 * actor_loss3)\n","\n","            # Optimize the actor \n","            self.actor_optimizer.zero_grad()\n","            actor_loss.backward()\n","            self.actor_optimizer.step()\n","\n","            # Update the frozen target models\n","            for param, target_param in zip(self.critic.parameters(), self.critic_target.parameters()):\n","              target_param.data.copy_(tau * param.data + (1 - tau) * target_param.data)\n","\n","            for param, target_param in zip(self.actor.parameters(), self.actor_target.parameters()):\n","              target_param.data.copy_(tau * param.data + (1 - tau) * target_param.data)\n","\n","    def save(self, filename):\n","        torch.save(self.critic.state_dict(), filename + \"_critic.pth\")\n","        torch.save(self.critic_optimizer.state_dict(), filename + \"_critic_optimizer.pth\")\n","\n","        torch.save(self.actor.state_dict(), filename + \"_actor.pth\")\n","        torch.save(self.actor_optimizer.state_dict(), filename + \"_actor_optimizer.pth\")\n","\n","        torch.save(self.sys_model.state_dict(), filename + \"_sys_model.pth\")\n","        torch.save(self.sys_model_optimizer.state_dict(), filename + \"_sys_model_optimizer.pth\")\n","\n","        torch.save(self.sys_reward.state_dict(), filename + \"sys_reward.pth\")\n","        torch.save(self.sys_reward_optimizer.state_dict(), filename + \"sys_reward_optimizer.pth\")\n","\n","    def load(self, filename):\n","        self.critic.load_state_dict(torch.load(filename + \"_critic.pth\"))\n","        self.critic_optimizer.load_state_dict(torch.load(filename + \"_critic_optimizer.pth\"))\n","        self.critic_target = copy.deepcopy(self.critic)\n","\n","        self.actor.load_state_dict(torch.load(filename + \"_actor.pth\"))\n","        self.actor_optimizer.load_state_dict(torch.load(filename + \"_actor_optimizer.pth\"))\n","        self.actor_target = copy.deepcopy(self.actor)\n","\n","        self.sys_model.load_state_dict(torch.load(filename + \"_sys_model.pth\"))\n","        self.sys_model_optimizer.load_state_dict(torch.load(filename + \"_sys_model_optimizer.pth\"))\n","\n","        self.sys_reward.load_state_dict(torch.load(filename + \"sys_reward.pth\"))\n","        self.sys_reward_optimizer.load_state_dict(torch.load(filename + \"sys_reward_optimizer.pth\"))"]},{"cell_type":"markdown","metadata":{"id":"GVQ1eB6Um2ir"},"source":["### Train"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7htMJKWlm1ld"},"outputs":[],"source":["def train(model, env_name, CER):\n","    ######### Hyperparameters #########\n","    model: str                    # TD3 or TD3_FORK\n","    env_name: str                 # BipedalWalker-v3 or BipedalWalkerHardcore-v3\n","    directory = 'drive/My Drive/Colab Notebooks/'\n","    device = torch.device('cpu')\n","    seed = 0                      # Sets Gym, PyTorch and Numpy seeds\n","    max_size = int(1e6)           # Max num of experiences in buffer\n","    batch_size = 256              # Batch size for both actor and critic\n","    discount = 0.99               # Discount factor \n","    lr = 3e-4                     # Learning rate for optimizers\n","    expl_noise = 0.3              # Std of Gaussian exploration noise\n","    decay = 0.995                 # Decay for expl_noise\n","    tau = 0.005                   # Target network update rate\n","    policy_noise = 0.2            # Noise added to target policy during critic update\n","    noise_clip = 0.5              # Range to clip target policy noise\n","    policy_freq = 2               # Frequency of delayed policy updates\n","    max_episodes = 500            # Max num of episodes\n","    max_timesteps = 2_000         # Max timesteps in one episode\n","    start_timestep = int(25e3)    # How many random actions before using policy\n","    video_every = 25              # Record video every x episodes\n","    \n","    ########### Initialise ###########\n","    display = Display(visible=0,size=(600,600))\n","    display.start()\n","\n","    env = gym.make(env_name)\n","    env = gym.wrappers.Monitor(env, \"./video\", video_callable=lambda ep_id: ep_id%video_every == 0, force=True)\n","    \n","    torch.manual_seed(seed)\n","    env.seed(seed)\n","    np.random.seed(seed)\n","    env.action_space.seed(seed)\n","\n","    state_dim = env.observation_space.shape[0]\n","    action_dim = env.action_space.shape[0]\n","    max_action = float(env.action_space.high[0])\n","    \n","    if model == 'TD3': policy = TD3(device, state_dim, action_dim, max_action, lr)\n","    elif model =='TD3_FORK': policy = TD3_FORK(device, state_dim, action_dim, max_action, lr)\n","    replay_buffer = ReplayBuffer(device, state_dim, action_dim, max_size)\n","\n","    log_f = open(directory+\"{}_log.txt\".format(env_name), \"w+\")\n","    ep_reward_list, total_timesteps = [], 0\n","    state, done, ep_reward = env.reset(), False, 0\n","\n","    ############# Training #############\n","    for episode in tqdm(range(max_episodes)):\n","        t0 = time.time()\n","        for t in range(max_timesteps):\n","\n","            # Update total_timesteps\n","            total_timesteps += 1\n","            if total_timesteps == start_timestep: print('started policy action sampling')\n","\n","            # Select action randomly\n","            if total_timesteps < start_timestep: \n","                action = env.action_space.sample()\n","            # Or according to policy\n","            else:\n","                action = policy.select_action(device, np.array(state))\n","                action = action + np.random.normal(0, max_action * expl_noise, size=action_dim)\n","                action = action.clip(-max_action, max_action)\n","            \n","            # Perform action and update reward\n","            next_state, reward, done, _ = env.step(action) \n","            ep_reward += reward\n","\n","            # Reward scaling\n","            if env_name == 'BipedalWalkerHardcore-v3':\n","                if reward == -100: reward = -5\n","                else: reward *= 5\n","\n","            # Store data in replay buffer and update state\n","            replay_buffer.add(state, action, next_state, reward, done)\n","            state = next_state\n","\n","            # Update observation bounds\n","            if model == 'TD3_FORK':\n","                policy.obs_upper_bound = np.amax(state) if policy.obs_upper_bound < np.amax(state) else policy.obs_upper_bound\n","                policy.obs_lower_bound = np.amin(state) if policy.obs_lower_bound > np.amin(state) else policy.obs_lower_bound\n","\n","            # Train agent after collecting sufficient random data\n","            if total_timesteps >= start_timestep:\n","                policy.train(device, replay_buffer, max_action, batch_size, discount, tau, policy_noise, noise_clip, policy_freq, CER)\n","\n","            if done:             \n","                # Store reward updates\n","                ep_reward_list.append(ep_reward)\n","    \n","                # Save logging updates\n","                print(\"Episode: {} \\t Reward: {} \\t Time Steps: {} \\t Time: {} \\t Expl Noise: {}\"\n","                      .format(episode, ep_reward, t+1, time.time()-t0, expl_noise))\n","                log_f.write('episode: {}, reward: {}\\n'.format(episode, ep_reward))\n","                log_f.flush()\n","\n","                # Expl_noise decay\n","                if ep_reward_list[-1] > 250: expl_noise *= decay\n","                if expl_noise < 0.1: expl_noise = 0.1\n","\n","                # # Save agent\n","                # policy.save(directory)\n","\n","                # Reset \n","                state, done, ep_reward = env.reset(), False, 0\n","\n","                # Show video and reward plots\n","                if episode % video_every == 0:\n","                    show_video(episode)\n","                    plot_results(ep_reward_list)\n","                    plt.show()\n","\n","                break\n","\n","    log_results(directory, ep_reward_list, '')"]},{"cell_type":"markdown","metadata":{"id":"rNlapDWHmJp3"},"source":["### Plot Results"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5iT_sMgImKZh"},"outputs":[],"source":["def plot():\n","    with open('drive/My Drive/Colab Notebooks/logs.json', 'r') as f:\n","        logs = json.load(f)\n","\n","    # Plot all trials for each model on different graphs\n","    for model in logs:\n","        ep_reward_lists = logs[model]\n","        for i, ep_reward_list in enumerate(ep_reward_lists):\n","            plot_results(ep_reward_list, label='', smooth=50, show_std=False)\n","        plt.show()\n","\n","    # Plot average of all trials for each model on one graph\n","    for model in logs:\n","        ep_reward_lists = logs[model]\n","        ep_reward_lists = [[float(sum(l))/len(l) for l in zip(*ep_reward_lists)]]\n","        for i, ep_reward_list in enumerate(ep_reward_lists):\n","            plot_results(ep_reward_list, label=model, smooth=50, show_std=False)\n","    plt.legend()\n","    plt.show()"]},{"cell_type":"markdown","source":["### Main"],"metadata":{"id":"ZxO4agJeBEfe"}},{"cell_type":"code","source":["train('TD3_FORK', 'BipedalWalker-v3', True)\n","# plot()"],"metadata":{"id":"OUl-vXrJBEMu"},"execution_count":null,"outputs":[]}],"metadata":{"colab":{"collapsed_sections":[],"name":"TD3/TD3_FORK.ipynb","provenance":[{"file_id":"1z3T970-48XwbFStSSPD-hEskm3zz2HYd","timestamp":1641651396031},{"file_id":"1S8tbUtwM-5peDF2BX5mtUqR154qsUwO6","timestamp":1641497536425},{"file_id":"173ZeNqInP-rgSAJp-wV_suOu-y9TXris","timestamp":1639480911540}],"authorship_tag":"ABX9TyNYfvy7/6fggrM13pQTkYZi"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}