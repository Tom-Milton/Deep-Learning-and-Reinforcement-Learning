{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"j3xbimrfORQ7"},"outputs":[],"source":["# DDPM: Using https://github.com/labmlai/annotated_deep_learning_paper_implementations\n","# DDPM: Specifically https://nn.labml.ai/diffusion/ddpm/index.html\n","# DDIM: Using https://github.com/ermongroup/ddim\n","\n","from typing import Tuple, Optional, Union, List\n","\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torch.utils.data as data\n","import torch.nn.functional as F\n","\n","import torchvision.utils as tvu\n","import torchvision.transforms as transforms\n","import torchvision.transforms.functional as TF\n","from torchvision.datasets import CIFAR10, STL10\n","\n","import sys\n","import imageio\n","import numpy as np\n","from tqdm.notebook import tqdm\n","from matplotlib import pyplot as plt"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":18557,"status":"ok","timestamp":1644453088161,"user":{"displayName":"thomas milton","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06460723225067031802"},"user_tz":0},"id":"D7D6rSnxlNlt","outputId":"7e68957d-1544-45d6-ee1f-a5429ad4a645"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"markdown","metadata":{"id":"OOeRomUGCTEV"},"source":["### U-Net"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QR_VrWOFCUXN"},"outputs":[],"source":["class Swish(nn.Module):\n","    def forward(self, x):\n","        return x * torch.sigmoid(x)\n","\n","\n","class TimeEmbedding(nn.Module):\n","    def __init__(self, n_channels: int):\n","        super().__init__()\n","        self.n_channels = n_channels\n","        self.lin1 = nn.Linear(self.n_channels // 4, self.n_channels)\n","        self.act = Swish()\n","        self.lin2 = nn.Linear(self.n_channels, self.n_channels)\n","\n","    def forward(self, t: torch.Tensor):\n","        half_dim = self.n_channels // 8\n","        emb = np.log(10_000) / (half_dim - 1)\n","        emb = torch.exp(torch.arange(half_dim, device=t.device) * -emb)\n","        emb = t[:, None] * emb[None, :]\n","        emb = torch.cat((emb.sin(), emb.cos()), dim=1)\n","        emb = self.act(self.lin1(emb))\n","        emb = self.lin2(emb)\n","        return emb\n","\n","\n","class ResidualBlock(nn.Module):\n","    def __init__(self, in_channels: int, out_channels: int, time_channels: int, n_groups: int = 32):\n","        super().__init__()\n","        self.norm1 = nn.GroupNorm(n_groups, in_channels)\n","        self.act1 = Swish()\n","        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=(3, 3), padding=(1, 1))\n","        self.norm2 = nn.GroupNorm(n_groups, out_channels)\n","        self.act2 = Swish()\n","        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=(3, 3), padding=(1, 1))\n","        if in_channels != out_channels:\n","            self.shortcut = nn.Conv2d(in_channels, out_channels, kernel_size=(1, 1))\n","        else:\n","            self.shortcut = nn.Identity()\n","        self.time_emb = nn.Linear(time_channels, out_channels)\n","\n","    def forward(self, x: torch.Tensor, t: torch.Tensor):\n","        h = self.conv1(self.act1(self.norm1(x)))\n","        h += self.time_emb(t)[:, :, None, None]\n","        h = self.conv2(self.act2(self.norm2(h)))\n","        return h + self.shortcut(x)\n","\n","\n","class AttentionBlock(nn.Module):\n","    def __init__(self, n_channels: int, n_heads: int = 1, d_k: int = None, n_groups: int = 32):\n","        super().__init__()\n","        if d_k is None: d_k = n_channels\n","        self.norm = nn.GroupNorm(n_groups, n_channels)\n","        self.projection = nn.Linear(n_channels, n_heads * d_k * 3)\n","        self.output = nn.Linear(n_heads * d_k, n_channels)\n","        self.scale = d_k ** -0.5\n","        self.n_heads = n_heads\n","        self.d_k = d_k\n","\n","    def forward(self, x: torch.Tensor, t: Optional[torch.Tensor] = None):\n","        _ = t\n","        batch_size, n_channels, height, width = x.shape\n","        x = x.view(batch_size, n_channels, -1).permute(0, 2, 1)\n","        qkv = self.projection(x).view(batch_size, -1, self.n_heads, 3 * self.d_k)\n","        q, k, v = torch.chunk(qkv, 3, dim=-1)\n","        attn = torch.einsum('bihd,bjhd->bijh', q, k) * self.scale\n","        attn = attn.softmax(dim=1)\n","        res = torch.einsum('bijh,bjhd->bihd', attn, v)\n","        res = res.view(batch_size, -1, self.n_heads * self.d_k)\n","        res = self.output(res)\n","        res += x\n","        res = res.permute(0, 2, 1).view(batch_size, n_channels, height, width)\n","        return res\n","\n","\n","class DownBlock(nn.Module):\n","    def __init__(self, in_channels: int, out_channels: int, time_channels: int, has_attn: bool):\n","        super().__init__()\n","        self.res = ResidualBlock(in_channels, out_channels, time_channels)\n","        if has_attn:\n","            self.attn = AttentionBlock(out_channels)\n","        else:\n","            self.attn = nn.Identity()\n","\n","    def forward(self, x: torch.Tensor, t: torch.Tensor):\n","        x = self.res(x, t)\n","        x = self.attn(x)\n","        return x\n","\n","\n","class UpBlock(nn.Module):\n","    def __init__(self, in_channels: int, out_channels: int, time_channels: int, has_attn: bool):\n","        super().__init__()\n","        self.res = ResidualBlock(in_channels + out_channels, out_channels, time_channels)\n","        if has_attn:\n","            self.attn = AttentionBlock(out_channels)\n","        else:\n","            self.attn = nn.Identity()\n","\n","    def forward(self, x: torch.Tensor, t: torch.Tensor):\n","        x = self.res(x, t)\n","        x = self.attn(x)\n","        return x\n","\n","\n","class MiddleBlock(nn.Module):\n","    def __init__(self, n_channels: int, time_channels: int):\n","        super().__init__()\n","        self.res1 = ResidualBlock(n_channels, n_channels, time_channels)\n","        self.attn = AttentionBlock(n_channels)\n","        self.res2 = ResidualBlock(n_channels, n_channels, time_channels)\n","\n","    def forward(self, x: torch.Tensor, t: torch.Tensor):\n","        x = self.res1(x, t)\n","        x = self.attn(x)\n","        x = self.res2(x, t)\n","        return x\n","\n","\n","class Upsample(nn.Module):\n","    def __init__(self, n_channels):\n","        super().__init__()\n","        self.conv = nn.ConvTranspose2d(n_channels, n_channels, (4, 4), (2, 2), (1, 1))\n","\n","    def forward(self, x: torch.Tensor, t: torch.Tensor):\n","        _ = t\n","        return self.conv(x)\n","\n","\n","class Downsample(nn.Module):\n","    def __init__(self, n_channels):\n","        super().__init__()\n","        self.conv = nn.Conv2d(n_channels, n_channels, (3, 3), (2, 2), (1, 1))\n","\n","    def forward(self, x: torch.Tensor, t: torch.Tensor):\n","        _ = t\n","        return self.conv(x)\n","\n","\n","class UNet(nn.Module):\n","    def __init__(self, image_channels: int = 3, n_channels: int = 64,\n","                 ch_mults: Union[Tuple[int, ...], List[int]] = (1, 2, 2, 4),\n","                 is_attn: Union[Tuple[bool, ...], List[int]] = (False, False, True, True),\n","                 n_blocks: int = 2):\n","        super().__init__()\n","        n_resolutions = len(ch_mults)\n","        self.image_proj = nn.Conv2d(image_channels, n_channels, kernel_size=(3, 3), padding=(1, 1))\n","        self.time_emb = TimeEmbedding(n_channels * 4)\n","\n","        down = []\n","        out_channels = in_channels = n_channels\n","        for i in range(n_resolutions):\n","            out_channels = in_channels * ch_mults[i]\n","            for _ in range(n_blocks):\n","                down.append(DownBlock(in_channels, out_channels, n_channels * 4, is_attn[i]))\n","                in_channels = out_channels\n","            if i < n_resolutions - 1:\n","                down.append(Downsample(in_channels))\n","        self.down = nn.ModuleList(down)\n","        self.middle = MiddleBlock(out_channels, n_channels * 4, )\n","\n","        up = []\n","        in_channels = out_channels\n","        for i in reversed(range(n_resolutions)):\n","            out_channels = in_channels\n","            for _ in range(n_blocks):\n","                up.append(UpBlock(in_channels, out_channels, n_channels * 4, is_attn[i]))\n","            out_channels = in_channels // ch_mults[i]\n","            up.append(UpBlock(in_channels, out_channels, n_channels * 4, is_attn[i]))\n","            in_channels = out_channels\n","            if i > 0:\n","                up.append(Upsample(in_channels))        \n","        self.up = nn.ModuleList(up)\n","        self.norm = nn.GroupNorm(8, n_channels)\n","        self.act = Swish()\n","        self.final = nn.Conv2d(in_channels, image_channels, kernel_size=(3, 3), padding=(1, 1))\n","\n","    def forward(self, x: torch.Tensor, t: torch.Tensor):\n","        t = self.time_emb(t)\n","        x = self.image_proj(x)\n","        h = [x]\n","        for m in self.down:\n","            x = m(x, t)\n","            h.append(x)\n","        x = self.middle(x, t)\n","        \n","        for m in self.up:\n","            if isinstance(m, Upsample):\n","                x = m(x, t)\n","            else:\n","                s = h.pop()\n","                x = torch.cat((x, s), dim=1)\n","                x = m(x, t)\n","        return self.final(self.act(self.norm(x)))"]},{"cell_type":"markdown","metadata":{"id":"YnepGIjKORQ_"},"source":["### Denoise Diffusion"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BzO5JiPXORRA"},"outputs":[],"source":["class DenoiseDiffusion(nn.Module):\n","    def __init__(self, eps_model: nn.Module, n_steps: int, device: torch.device):\n","        super().__init__()\n","        self.eps_model = eps_model\n","        self.n_steps = n_steps\n","        self.device = device\n","\n","        self.beta = torch.linspace(0.0001, 0.02, n_steps).to(device)\n","        self.alpha = 1. - self.beta\n","        self.alpha_bar = torch.cumprod(self.alpha, dim=0)\n","    \n","    def gather(self, consts: torch.Tensor, t: torch.Tensor):\n","        \"\"\"Gather consts for t and reshape to feature map shape\"\"\"\n","        c = consts.gather(-1, t).reshape(-1, 1, 1, 1)\n","        return c.reshape(-1, 1, 1, 1)\n","\n","    def p_x0(self, xt: torch.Tensor, t: torch.Tensor):\n","        \"\"\"Estimate x0\"\"\"\n","        eps_theta = self.eps_model(xt, t)\n","        alpha_bar = self.gather(self.alpha_bar, t)\n","        return (xt - (1 - alpha_bar) ** 0.5 * eps_theta) / (alpha_bar ** 0.5)\n","\n","    def q_sample(self, x0: torch.Tensor, t: torch.Tensor, eps: Optional[torch.Tensor] = None):\n","        \"\"\"Sample from q(xt|x0)\"\"\"\n","        if eps is None: eps = torch.randn_like(x0)\n","        mean = self.gather(self.alpha_bar, t) ** 0.5 * x0\n","        var = 1 - self.gather(self.alpha_bar, t)\n","        return mean + (var ** 0.5) * eps\n","\n","    def p_sample(self, xt: torch.Tensor, t: torch.Tensor):\n","        \"\"\"Sample from p_theta(xt_1|xt)\"\"\"\n","        eps_theta = self.eps_model(xt, t)\n","        alpha_bar = self.gather(self.alpha_bar, t)\n","        alpha = self.gather(self.alpha, t)\n","        eps_coef = (1 - alpha) / (1 - alpha_bar) ** .5\n","        mean = 1 / (alpha ** 0.5) * (xt - eps_coef * eps_theta)\n","        var = self.gather(self.beta, t)\n","        eps = torch.randn(xt.shape, device=xt.device)\n","        return mean + (var ** .5) * eps\n","\n","    def loss(self, x0: torch.Tensor, noise: Optional[torch.Tensor] = None):\n","        \"\"\"Simplified loss\"\"\"\n","        batch_size = x0.shape[0]\n","        t = torch.randint(0, self.n_steps, (batch_size,), device=x0.device, dtype=torch.long)\n","        if noise is None: \n","            noise = torch.randn_like(x0)\n","        xt = self.q_sample(x0, t, eps=noise)\n","        eps_theta = self.eps_model(xt, t)\n","        return F.mse_loss(noise, eps_theta)\n","\n","    def generalized_steps(self, x: torch.Tensor, seq: list):\n","        \"\"\"From https://github.com/ermongroup/ddim\"\"\"\n","        alpha_bar = torch.cat([torch.ones(1).to(self.device), self.alpha_bar], dim=0)\n","        n = x.size(0)\n","        seq_next = [-1] + list(seq[:-1])\n","        x0_preds = []\n","        xs = [x]\n","\n","        for i, j in tqdm(zip(reversed(seq), reversed(seq_next)), total=len(seq), leave=None):\n","            t = (torch.ones(n) * i).to(self.device)\n","            next_t = (torch.ones(n) * j).to(self.device)\n","            at = alpha_bar.index_select(0, t.long()+1).view(-1, 1, 1, 1)\n","            at_next = alpha_bar.index_select(0, next_t.long()+1).view(-1, 1, 1, 1)\n","\n","            xt = xs[-1].to(self.device)\n","            et = self.eps_model.forward(xt, t)\n","            x0_t = (xt - et * (1 - at).sqrt()) / at.sqrt()\n","            x0_preds.append(x0_t.cpu())\n","\n","            c1 = (0 * ((1 - at / at_next) * (1 - at_next) / (1 - at)).sqrt())\n","            c2 = ((1 - at_next) - c1 ** 2).sqrt()\n","            xt_next = at_next.sqrt() * x0_t + c1 * torch.randn_like(x) + c2 * et\n","            xs.append(xt_next.cpu())\n","\n","        return xs, x0_preds"]},{"cell_type":"markdown","metadata":{"id":"fhT6eMAiDcjd"},"source":["### Sampler"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7eAvRA1tCDn1"},"outputs":[],"source":["class Sampler(nn.Module):\n","    def __init__(self, diffusion: DenoiseDiffusion, image_channels: int, image_size: int, device: torch.device):\n","        super().__init__()\n","        self.diffusion = diffusion\n","        self.n_steps = diffusion.n_steps\n","        \n","        self.device = device\n","        self.image_channels = image_channels\n","        self.image_size = image_size\n","    \n","    def make_grid(self, x: torch.Tensor, nrow: int):\n","        \"\"\"Helper function to create a grid of images\"\"\"\n","        plt.imshow(tvu.make_grid(x, nrow=nrow, padding=2).permute(1, 2, 0))\n","        plt.axis('off')\n","        plt.show()\n","\n","    def slerp(self, z1: torch.Tensor, z2: torch.Tensor, alpha: torch.Tensor):\n","        \"\"\"Spherical linear interpolation\"\"\"\n","        theta = torch.acos(torch.sum(z1 * z2) / (torch.norm(z1) * torch.norm(z2)))\n","        return (torch.sin((1 - alpha) * theta) / torch.sin(theta) * z1 + torch.sin(alpha * theta) / torch.sin(theta) * z2)\n","    \n","    def DDPM_sampling(self, xt: torch.Tensor, n_steps: int=1000):\n","        \"\"\"Generate bath of images using DDPM sampling\"\"\"\n","        for t_inv in tqdm(range(n_steps), leave=None):\n","            t_ = n_steps - t_inv - 1\n","            t = xt.new_full((xt.size(0),), t_, dtype=torch.long)\n","            xt = self.diffusion.p_sample(xt, t)\n","\n","        return xt\n","\n","    def DDPM_sequence(self, xt: torch.Tensor, n_frames: int):\n","        \"\"\"Sample an image step-by-step using p_theta(xt_1|xt)\"\"\"\n","        # Calculate which diffusion indices to show\n","        intervals = [int(i) for i in np.linspace(0, self.n_steps-1, n_frames)]; intervals.pop(0)\n","        \n","        frames = []\n","        # Get diffusion sequence for each noise image in batch\n","        for i in tqdm(range(xt.size(0)), leave=None):\n","            # Add initial noise to sequence\n","            sequence = [torch.squeeze(xt[i]).cpu()]\n","            # Perform reverse (generative) process for all T steps\n","            for t_inv in tqdm(range(self.n_steps), leave=None):\n","                t_ = self.n_steps - t_inv - 1\n","                t = xt[i].new_full((1,), t_, dtype=torch.long)\n","                # Save intermediate image to sequence\n","                if t_inv in intervals:\n","                    x0 = self.diffusion.p_x0(xt[i][None, :, :, :], t)\n","                    sequence.append(x0[0].cpu())\n","                # Perform reverse process\n","                xt[i] = self.diffusion.p_sample(xt[i][None, :, :, :], t)\n","            # Add to frames\n","            frames += sequence\n","\n","        return frames\n","\n","    def DDPM_interpolation(self, x1: torch.Tensor, x2: torch.Tensor, n_frames: int, t_: int = 100):\n","        \"\"\"Interpolate two images x1 and x2 step by step\"\"\" \n","        t = torch.full((1,), t_, device=self.device)\n","\n","        frames = []\n","        # Get inteprolation sequence for each noise image in batch\n","        for i in tqdm(range(len(x1)), leave=None):\n","            # Forward process for t steps (larger t = more coarse)\n","            x1t = self.diffusion.q_sample(x1[i][None, :, :, :], t)\n","            x2t = self.diffusion.q_sample(x2[i][None, :, :, :], t)\n","\n","            # Add initial image1 to sequence\n","            sequence = [torch.tensor(x1[i]).cpu()]\n","            # Performe linear interpolation between the images\n","            for j in tqdm(range(1, n_frames-1), leave=None):\n","                # Perform linear interpolation \n","                lambda_ = j / (n_frames-1)\n","                xt = (1 - lambda_) * x1t + lambda_ * x2t\n","                # Decode latents into image space\n","                x0 = self.DDPM_sampling(xt, t_)\n","                sequence.append(x0[0].cpu())\n","            # Add initial image2 to sequence\n","            sequence.append(torch.tensor(x2[i]).cpu())\n","            # Add to frames\n","            frames += sequence\n","\n","        return frames\n","\n","    def DDIM_sampling(self, xt: torch.Tensor, n_timesteps: int):\n","        \"\"\"Based on the implementation in https://github.com/ermongroup/ddim\"\"\"\n","        seq = range(0, self.n_steps, self.n_steps // n_timesteps)\n","        x = self.diffusion.generalized_steps(xt, seq)\n","        return x\n","\n","    def DDIM_sequence(self, xt: torch.Tensor, n_frames: int, n_timesteps: int):\n","        \"\"\"Based on the implementation in https://github.com/ermongroup/ddim\"\"\"\n","        # Calculate which diffusion indices to show\n","        intervals = [int(i) for i in np.linspace(0, n_timesteps-1, n_frames)]\n","\n","        frames = []\n","        # Get diffusion sequence for each noise image in batch\n","        for i in tqdm(range(xt.size(0)), leave=None):\n","            # Get entire diffusion sequence\n","            x0_preds = self.DDIM_sampling(xt[i][None, :, :, :], n_timesteps)[1]\n","            # Select images with indices from intervals\n","            sequence = [torch.squeeze(x0_preds[j]) for j in range(n_timesteps) if j in intervals]\n","            # Add initial noise instead of first frame\n","            sequence[0] = torch.squeeze(xt[i]).cpu()\n","            # Add to frames\n","            frames += sequence\n","\n","        return frames\n","\n","    def DDIM_interpolation(self, z1: torch.Tensor, z2: torch.Tensor, n_frames: int, n_timesteps: int):\n","        \"\"\"Based on the implementation in https://github.com/ermongroup/ddim\"\"\"\n","        # Calculate interpolation points\n","        alpha = torch.linspace(0, 1, n_frames).to(self.device)\n","\n","        frames = []\n","        # Get inteprolation sequence for each noise image in batch\n","        for i in tqdm(range(z1.size(0)), leave=None):\n","            # Interpolate noise using slerp\n","            x = torch.cat([self.slerp(z1[i][None, None, :, :, :], z2[i][None, None, :, :, :], alpha[j]) for j in range(n_frames)], dim=0)\n","            # Generate images from noise at interpolation points\n","            sequence = [torch.squeeze(self.DDIM_sampling(x[j], n_timesteps)[0][-1]) for j in tqdm(range(n_frames), leave=None)]\n","            # Add to frames\n","            frames += sequence\n","            \n","        return frames"]},{"cell_type":"markdown","metadata":{"id":"tDxAQrXNORRE"},"source":["### Configs"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BkZdMuRiORRE"},"outputs":[],"source":["class Configs:\n","    device: torch.device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n","    eps_model: UNet\n","    diffusion: DenoiseDiffusion\n","    sampler: Sampler\n","    dataset_name: str = 'cifar10'  # cifar10 or stl10\n","    image_channels: int = 3\n","    image_size: int = 32  # 32 or 96\n","    n_channels: int = 64\n","    channel_multipliers: List[int] = [1, 2, 2, 4]\n","    is_attention: List[int] = [False, False, False, True]\n","    n_steps: int = 1_000\n","    batch_size: int = 64  # 64 or 16\n","    learning_rate: float = 2e-5\n","    epoch: int = 1\n","    dataset: data.Dataset\n","    data_loader: data.DataLoader\n","    optimizer: optim.Adam\n","\n","    def init(self):\n","        self.eps_model = UNet(image_channels=self.image_channels, n_channels=self.n_channels, ch_mults=self.channel_multipliers, is_attn=self.is_attention).to(self.device)\n","        self.diffusion = DenoiseDiffusion(eps_model=self.eps_model, n_steps=self.n_steps, device=self.device)\n","        self.sampler = Sampler(self.diffusion, image_channels=self.image_channels, image_size=self.image_size, device=self.device)\n","\n","        if self.dataset_name == 'cifar10':\n","            self.dataset = CIFAR10(root='./data', download=True, transform=transforms.ToTensor())\n","        \n","        if self.dataset_name == 'stl10':\n","            self.dataset1 = STL10(root='./data', split='train', download=True, transform=transforms.ToTensor())\n","            self.dataset2 = STL10(root='./data', split='test', download=True, transform=transforms.ToTensor())\n","            self.dataset = data.ConcatDataset([self.dataset1, self.dataset2])  # Join test and train datasets\n","            \n","        self.data_loader = data.DataLoader(self.dataset, self.batch_size, shuffle=True, pin_memory=True)\n","        self.optimizer = optim.Adam(self.eps_model.parameters(), lr=self.learning_rate)\n","\n","        # Load previous parameters from drive\n","        params = torch.load('drive/My Drive/Colab Notebooks/{}.chkpt'.format(self.dataset_name), map_location=self.device)\n","        self.diffusion.load_state_dict(params['diffusion'])\n","        self.optimizer.load_state_dict(params['optimizer'])\n","        self.epoch = params['epoch']\n","    \n","    def train(self):\n","        while True:\n","            print('epoch:', self.epoch)\n","            self.epoch += 1\n","\n","            # Train\n","            for i, (x, y) in enumerate(tqdm(self.data_loader, leave=None)):\n","                data_ = x.to(self.device)\n","                self.optimizer.zero_grad()\n","                loss = self.diffusion.loss(data_)\n","                loss.backward()\n","                self.optimizer.step()\n","\n","            # Sample\n","            with torch.no_grad():\n","                noise = torch.randn([64, self.image_channels, self.image_size, self.image_size], device=self.device)\n","                x = self.sampler.DDIM_sampling(noise, n_timesteps=10)[0][-1]\n","                self.sampler.make_grid(x, nrow=int(np.sqrt(x.size(0))))\n","\n","            # Save parameters to drive\n","            torch.save({'diffusion': self.diffusion.state_dict(),\n","                        'optimizer': self.optimizer.state_dict(), \n","                        'epoch': self.epoch},\n","                        'drive/My Drive/Colab Notebooks/{}.chkpt'.format(self.dataset_name))\n","\n","    def sample(self):\n","        with torch.no_grad():\n","            # Generate batches of random noise for sampling\n","            xt1 = torch.randn([64, self.image_channels, self.image_size, self.image_size], device=self.device)\n","            xt2 = torch.randn([1, self.image_channels, self.image_size, self.image_size], device=self.device)\n","            z1 = torch.randn([8, self.image_channels, self.image_size, self.image_size], device=self.device)\n","            z2 = torch.randn([8, self.image_channels, self.image_size, self.image_size], device=self.device)\n","\n","            # DDIM sampling\n","            x = self.sampler.DDIM_sampling(xt1, n_timesteps=10)[0][-1]\n","            self.sampler.make_grid(x, nrow=int(np.sqrt(x.size(0))))\n","            x = self.sampler.DDIM_sampling(xt1, n_timesteps=1000)[0][-1]\n","            self.sampler.make_grid(x, nrow=int(np.sqrt(x.size(0))))\n","\n","            frames1 = self.sampler.DDIM_sequence(xt2, n_frames=20, n_timesteps=100)\n","            self.sampler.make_grid(frames1, nrow=len(frames1)//xt2.size(0))\n","\n","            frames2 = self.sampler.DDIM_interpolation(z1, z2, n_frames=8, n_timesteps=100)\n","            self.sampler.make_grid(frames2, nrow=len(frames2)//z1.size(0))\n","\n","            # Get generated images from DDIM interpolation for DDPM interpolation\n","            x1 = [frames2[i].to(self.device) for i in range(len(frames2)) if i%8==0]\n","            x2 = [frames2[i].to(self.device) for i in range(len(frames2)) if (i+1)%8==0]\n","\n","            # DDPM sampling\n","            x = self.sampler.DDPM_sampling(xt1).cpu()\n","            self.sampler.make_grid(x, nrow=int(np.sqrt(x.size(0))))\n","\n","            frames1 = self.sampler.DDPM_sequence(xt2, n_frames=20)\n","            self.sampler.make_grid(frames1, nrow=len(frames1)//xt2.size(0))\n","\n","            frames2 = self.sampler.DDPM_interpolation(x1, x2 , n_frames=8, t_=100)\n","            self.sampler.make_grid(frames2, nrow=len(frames2)//len(x1))\n","            frames2 = self.sampler.DDPM_interpolation(x1, x2 , n_frames=8, t_=500)\n","            self.sampler.make_grid(frames2, nrow=len(frames2)//len(x1))"]},{"cell_type":"markdown","metadata":{"id":"tue9vNIOocss"},"source":["### Main"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5JQ1Pu8LIQWC"},"outputs":[],"source":["configs = Configs()\n","configs.init()\n","# configs.train()\n","configs.sample()"]}],"metadata":{"colab":{"collapsed_sections":[],"name":"DDPM/DDIM.ipynb","provenance":[{"file_id":"1luN7ECbVIP9-2V7kxxebsU-lbxzk-pHp","timestamp":1641398119914},{"file_id":"1W49N2MLB72nTCHg16UTsaQz1dLLY0Sfh","timestamp":1640003669585}]},"interpreter":{"hash":"c1df07f34c869bd847fe96534ac0e0bef4ded485c0c4eca2942d7b85f5b7823d"},"kernelspec":{"display_name":"Python 3.8.0 64-bit ('venv': venv)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.0"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}